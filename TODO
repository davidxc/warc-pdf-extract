- make item-level task non-local
    => unified crawl metadata as separate download+cat task
- refactor crawl-specific luigi process
    => download single warc
    => iterate over contents
        => if PDF, POST to grobid service
        => write (.tei.xml) results to WARC-specific directory
        => write a stamp when complete
    => zip up results and upload to petabox
- seed-list-based luigi/hadoop process
    => chunk seedlist into item-sized chunks, then task-sized chunks
        => chunks of 100k PDFs per item (10GB uncompressed raw text)
        => 50x tasks of 2000 PDFs (~10 minutes of work, if fast)
        => 500 tasks per million URLs
    => download files to directory; create manifest of missing and file output
    => run grobid on directory (rest is same as above)

While i'm at it, could...
x metamgr-like pulling of item lists
    => using ia-mine
- update luigi install process to use gluish upstream
- ia wrapper luigi tasks (eg, check if remote item/file exists)
